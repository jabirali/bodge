import os
from multiprocessing import Pool, cpu_count
from typing import Iterator

import numpy as np
import numpy.typing as npt
import scipy.sparse as sp
from h5py import File
from tqdm import trange

from .consts import *
from .lattice import *
from .physics import *

Energy = np.float64
Weight = np.float64
SpectralValue = sp.bsr_matrix
SpectralTuple = tuple[SpectralValue, Energy, Weight]


class SpectralSolution:
    """Wrapper around the HDF5 solutions files generated by `SpectralSolver`."""

    def __init__(self, file_name: str):
        # Open the represented HDF5 file.
        self.file: File = File(file_name, "r")

    def __del__(self):
        # Close the represented HDF5 file.
        self.file.close()

    def __iter__(self) -> Iterator[SpectralTuple]:
        """Iterate over all stored results."""
        spectral = self.file["/spectral/"]
        for m in spectral:
            # Reconstruct spectral function.
            data = spectral[f"{m}/data"]
            indptr = spectral[f"{m}/indptr"]
            indices = spectral[f"{m}/indices"]

            A_m: SpectralValue = sp.bsr_matrix((data, indices, indptr))

            # Extract remaining variables.
            ω_m: Energy = spectral[f"{m}/energy"][...]
            w_m: Weight = spectral[f"{m}/weight"][...]

            yield A_m, ω_m, w_m


class SpectralSolver:
    """Defines an API for numerically calculating spectral functions.

    This solver assumes the following properties for the derived solvers:

    - Local Krylov subspace methods are used to obtain linear scaling;
    - The Hamiltonian and spectral function are implemented as BSR matrices;
    - Parallelization based on the independent expansion of matrix blocks;
    - These blocks are saved as HDF5 files to limit the RAM requirements.

    To use this class, subclass it and implement the missing solver method.
    """

    def __init__(
        self,
        hamiltonian: Hamiltonian,
        processes: Optional[int] = None,
        blocksize: int = 1024,
        radius: int = 4,
    ):
        # Reference to the sparse matrix we use.
        self.hamiltonian: sp.bsr_matrix = hamiltonian.matrix
        self.skeleton: sp.bsr_matrix = hamiltonian.struct

        # Linear scaling is achieved via a Local Krylov cutoff.
        self.radius: int = radius
        if self.radius < 1:
            raise RuntimeError("Krylov cutoff radius must be a positive integer.")

        # Parallelization is done by division into matrix blocks.
        self.blocksize: int = blocksize
        self.blocks: int = self.hamiltonian.shape[1] // blocksize
        if self.blocksize * self.blocks != hamiltonian.shape[1]:
            raise RuntimeError(f"Hamiltonian shape must be a multiple of {blocksize}.")

        # Parallel processes is by default we use the number of processor cores (min 2).
        if processes is not None:
            self.processes: int = processes
        else:
            self.processes: int = max(cpu_count(), 2)

        # Declare additional variables for methods and subclasses to define.
        self.energies: npt.NDArray[Energy]
        self.weights: npt.NDArray[Weight]

        self.block: int
        self.block_name: str
        self.block_identity: sp.bsr_matrix
        self.block_neighbors: sp.bsr_matrix
        self.block_subspace: sp.bsr_matrix

    def __call__(self, block: Optional[int] = None):
        """Calculate the spectral function as a function of energy.

        After instantiating this class as e.g. `solver`, run `solver()` in
        the `__main__` process to calculate the complete spectral function.

        Calling the instance as `solver(k)` calculates block number k, and
        is meant to be run by individial worker processes in parallel. The
        solver should usually not be used in this manner in user scripts.
        """
        if block is None:
            # Calculate each block A_km = [A_k(ω_m)] of the spectral function
            # A(ω) in parallel. The results are stored as HDF5 to save RAM.
            print("[green]:: Calculating the spectral function in parallel[/green]")
            with Pool(self.processes) as pool:
                block_range = trange(self.blocks, desc=" -> expanding", unit="block")
                block_names = sorted(pool.imap_unordered(self, block_range))

            # Open the generated HDF5 files for reading, and merge the blocks
            # [A_k(ω_m)] into complete matrices A(ω_m). The results are
            # written to a new output file which is also stored as HDF5.
            result_name = "bodge.hdf"
            with File(result_name, "w", rdcc_nbytes=1024**3) as result_file:
                # Iterate over every energy ω_m.
                result_range = trange(len(self.energies), desc=" -> merging", unit="energy")
                for m in result_range:
                    A_m = []
                    # Iterate over every block A_k(ω_m).
                    for block_name in block_names:
                        with File(block_name, "r") as block_file:
                            # Extract data from corresponding input files.
                            data = block_file[f"/block/{m:04d}/data"]
                            indices = block_file[f"/block/{m:04d}/indices"]
                            indptr = block_file[f"/block/{m:04d}/indptr"]

                            # Reconstruct the sparse matrix A_k(ω_m).
                            A_km = bsr_matrix((data, indices, indptr))

                            # Save this matrix for further processing.
                            A_m.append(A_km)

                    # Merge all matrix blocks A_k(ω_m) into one matrix A(ω_m).
                    A_m = sp.hstack(A_m, "bsr")

                    # Decontruct the matrix and store in the output file.
                    result_file[f"/spectral/{m:04d}/indices"] = A_m.indices
                    result_file[f"/spectral/{m:04d}/indptr"] = A_m.indptr
                    result_file[f"/spectral/{m:04d}/data"] = A_m.data

                # Close and remove the input files after processing.
                print("-> cleaning up temporary files")
                for block_name in block_names:
                    os.remove(block_name)

                # Save other relevant variables.
                print("-> saving auxilliary variables")
                for m in range(len(self.energies)):
                    result_file[f"/spectral/{m:04d}/energy"] = self.energies[int(m)]
                    result_file[f"/spectral/{m:04d}/weight"] = self.weights[int(m)]

            # Return the generated output file.
            print("--> done!")
            return SpectralSolution(result_name)
        else:
            # Calculate the spectral function A_k(ω_m) for a block index k.
            # The results should be stored in an HDF5 file `block_name`,
            # which is returned to the caller after the calculation.
            self.block_init(block)
            with File(self.block_name, "w", rdcc_nbytes=1024**3) as block_file:
                # Prepare an empty skeleton for storing the results.
                block_template = sp.bsr_matrix(self.block_neighbors, dtype=np.complex128)
                block_template.data *= 0

                # Store the skeletons to the output file.
                A_k = {}
                for m in range(len(self.energies)):
                    # Save one BSR matrix for each block A_k(ω_m).
                    block_file[f"/block/{m:04d}/indices"] = block_template.indices
                    block_file[f"/block/{m:04d}/indptr"] = block_template.indptr
                    block_file[f"/block/{m:04d}/data"] = block_template.data

                    # Save a reference to its data for easy updates.
                    A_k[m] = block_file[f"/block/{m:04d}/data"]

                # Perform calculations for this block in working area `A_k`.
                # This ensures that `block_solve` doesn't need to handle the
                # interaction with the HDF5 temporary file explicitly.
                self.block_solve(A_k)

            return self.block_name

    def block_init(self, block: int) -> None:
        """Prepare for performing calculations at a given block index.

        This instantiates all the matrices required for a linear-scaling
        polynomial expansion of the spectral function at the given block.
        """
        # Save the block index.
        self.block = block

        # Instantiate the current block of the identity matrix.
        diag = np.repeat(np.int8(1), self.blocksize)
        offset = -block * self.blocksize
        shape = (self.hamiltonian.shape[0], self.blocksize)
        identity = sp.dia_matrix((diag, [offset]), shape, dtype=np.int8)

        self.block_identity = identity.tobsr(self.hamiltonian.blocksize)

        # Projection with this mask retains only local terms (up to nearest
        # neighbors), which are the relevant terms in the spectral function.
        self.block_neighbors = self.skeleton @ self.block_identity

        # Projection with this mask retains all terms within a "bubble" of
        # a given radius. This defines the Local Krylov subspace used for
        # intermediate calculations in the Green function expansions.
        mask = self.block_neighbors
        for _ in range(self.radius - 1):
            mask = self.skeleton @ mask
        mask.data[...] = 1

        self.block_subspace = sp.bsr_matrix(mask, dtype=np.int8)

        # Prepare a filename where the results can be stored.
        self.block_name = f"block_{self.block:08d}.hdf"

    def block_solve(self, A_k: dict[int, ArrayLike]) -> None:
        raise NotImplementedError


class ChebyshevSolver(SpectralSolver):
    """Chebyshev expansion of spectral functions."""

    def __init__(self, *args, order: int = 200, **kwargs):
        # Superclass constructor.
        super().__init__(*args, **kwargs)

        # Chebyshev nodes {ω_m} where we calculate the spectral function
        # and corresponding weights {w_m} used for quadrature integration.
        N = order
        k = np.arange(2 * N)
        ω = np.cos(π * (2 * k + 1) / (4 * N))
        w = N * π * np.sqrt(1 - ω**2)

        # Calculate the corresponding Chebyshev transform coefficients.
        # TODO: Incorporate the relevant Lorentz kernel factors here.
        n = np.arange(N)
        T = np.cos(n[None, :] * np.arccos(ω[:, None])) / N
        T[:, 1:] *= 2

        # Save relevant variables internally.
        self.order = order
        self.chebyshev = T
        self.energies = ω
        self.weights = w

    def block_solve(self, storage):
        """Chebyshev expansion of a given block of the spectral function."""
        # Compact notation for the essential matrices.
        H = self.hamiltonian
        T = self.chebyshev

        I_k = self.block_identity
        P_k = self.block_neighbors
        R_k = self.block_subspace

        # Initialize the first two Chebyshev matrix blocks A_kn, and calculate
        # the corresponding contributions to the spectral function A_k(ω_m).
        A_k0 = I_k
        A_k1 = H @ I_k

        # Prepare a storage file for this block, and store the initial results.
        A_k = {}
        for m in storage:
            A_k[m] = T[m, 0] * A_k0 + T[m, 1] * A_k1

        # Chebyshev expansion of the next elements.
        for n in range(2, self.order):
            # Chebyshev expansion of next vector. Element-wise multiplication
            # by R_k projects the result back into the Local Krylov subspace.
            A_k1, A_k0 = (H @ A_k1).multiply(R_k) - A_k0, A_k1

            # Perform the Chebyshev transformation. Element-wise multiplication
            # by H_k preserves only on-site and nearest-neighbor interactions.
            # WARNING: This has been optimized to ignore SciPy wrapper checks.
            AH_kn = A_k1.multiply(P_k)
            for m, A_km in A_k.items():
                A_k[m].data += T[m, n] * AH_kn.data

        # Scale the final results using the integral weights.
        for m, A_km in A_k.items():
            A_km.data /= self.weights[m]

        # Copy out results.
        for m, A_km in A_k.items():
            storage[m] = A_km.data
